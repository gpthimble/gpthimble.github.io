{"pages":[{"title":"404 NOT FOUND","text":"","link":"/404.html"},{"title":"Categories","text":"","link":"/categories/index.html"},{"title":"about","text":"","link":"/about/index.html"},{"title":"Tags","text":"","link":"/tags/index.html"},{"title":"404 NOT FOUND","text":"","link":"/404.html"},{"title":"about","text":"","link":"/zh-CN/about/index.html"},{"title":"Categories","text":"","link":"/zh-CN/categories/index.html"},{"title":"Tags","text":"","link":"/zh-CN/tags/index.html"}],"posts":[{"title":"cpu内部只采用一个时钟，对于不同速度的组件将采用clock enabler","text":"几乎所有人都不建议，在FPGA中使用多于一个时钟。在FPGA中使用不同的时钟，涉及到在不同的时钟域（clock domain）进行同步，数据的同步通常使用两个串联的触发器，flag（一个周期的信号）通常转换成电平转换，然后再到另一个域进行同步，详细的设计可以看这里。 因此，我也决定在我的设计中撤销异步的清零、下边沿的清零。在需要使用不同速度的组件，如设计目标中的可以暂停的，可以调速的cpu，采用计数器激活的clock enabler，详细设计可以看这里。这样对我现有的设计的修改产生了一定的工作量。","link":"/cpu%E5%86%85%E9%83%A8%E5%8F%AA%E9%87%87%E7%94%A8%E4%B8%80%E4%B8%AA%E6%97%B6%E9%92%9F%EF%BC%8C%E5%AF%B9%E4%BA%8E%E4%B8%8D%E5%90%8C%E9%80%9F%E5%BA%A6%E7%9A%84%E7%BB%84%E4%BB%B6%E5%B0%86%E9%87%87%E7%94%A8clock-enabler/"},{"title":"下一步的计划","text":"现在lcc的代码已读完，下一步要做的是soc的设计和验证，以及lcc的移植、汇编器的移植。对于汇编器，我准备使用customasm.如果使用这个汇编器，可能在移植lcc时需要修改lcc模板中的一些伪指令如ld,lcomm等。 对于soc的设计，这些天一直在思考的问题是异步通讯的问题。我所希望的系统，cpu运行的速度应该是可变的，甚至可以是单步的。而我又希望能使用de0 nao板载的SDRAM，因此内存控制系统与cpu的通讯一定是异步的。异步通讯要有ready信号，由于我希望cpu运行的速度可变，因此我需要在cache控制器中设置一个双触发的触发器，在ready信号变化时记录并保持ready信号，直至cpu的下一个周期到来，用这个信号控制cpu中cache控制器和ram控制器的异步通讯。 系统地址空间的划分，也是一个需要确定的设计。目前我计划将地址空间按照| ROM | IO | RAM |划分，具体各区域尺寸尚未确定。系统cache只对RAM区域有效。目前我的想法是，根据指令和数据需求的不同，在cache控制器上增加片选、控制信号mux，以实现不同区域对cache的不同操作。对于指令，只需要读取ROM和RAM，所以I cache只需要在原有I cache上增加对ROM的多路复用（直通），由于这个ROM是用作bootloader的，因此不需要很大体积，可以使用同步的方式与I cache链接，因此I cache的改动很小。对于数据，则三个区域都需要读取。显然，io与cpu之间的通讯也是异步的，但我尚未构思完毕整个io控制器的结构，因此io控制器的设计应该是接下来的工作之一。 IO控制器应该包括一些总线（I2C，SPI，etc），一些通讯端口（GPIO，Uart，etc），以及向量中断和异步通讯的支持。需要仔细考虑。 我目前已有cpu部分的Verilog代码，但是这部分代码缺乏测试，没有精确中断支持，没有与cache集成，而且一些指令还需要修改，例如：我打算删除乘法和除法的硬件支持，原因是原有的代码中对乘法和除法的硬件支持来自Altera的IP，这与本项目的设计目标冲突，而且在sparc平台的lcc后端中，有使用软件支持乘法除法的参考样例；我还需要考虑是否增加不同长度的取数操作，考虑如果在lcc移植过程中将所有整数类型的尺寸或者对齐都设置为4字节会怎样。 但是这些工作的顺序很难确定，这是我首先要考虑的问题。 目前我想到的合理的顺序是： 考虑是否增加不同长度的取数操作 回顾已有的cpu代码，分析那些代码可以与cache、IO、中断等独立，并对这些代码进行测试。 定义存储控制和io控制的接口，与cpu对接，测试cpu的中断 设计存储控制 设计io控制 工具链的移植","link":"/%E4%B8%8B%E4%B8%80%E6%AD%A5%E7%9A%84%E8%AE%A1%E5%88%92/"},{"title":"关于中断和io控制器","text":"io和中断的设计定型，成了目前我的cpu设计过程中最亟待解决的问题。若不定型，可能影响接下来其他组件的设计和测试工作。 中断控制基本没啥可修改的，cpu端只有一个中断脚，cp0（协处理器0，即cu）中有四个寄存器，cause用于记录外部中断、内部中断和异常的种类；status中控制这一个中断的开关；base记录中断处理程序的地址，在中断处理程序中使用mfc0指令查询cause寄存器跳转到相应中断的处理程序地址；EPC用来保存中断前的指令地址。这一部分，我只需要额外实现三条指令mfc0，mtc0和eret。 io模块的输入输出部分应该是一个寄存器组，包括状态寄存器、指令寄存器、数据寄存器。状态寄存器标志忙闲，指令寄存器为从cpu获取指令，数据寄存器作为io控制器输入输出的通道。此外，io模块还要与cause寄存器相连，辅助一套中断屏蔽与优先级逻辑，完成中断向量。IO模块中的中断控制可以级联。","link":"/%E5%85%B3%E4%BA%8E%E4%B8%AD%E6%96%AD%E5%92%8Cio%E6%8E%A7%E5%88%B6%E5%99%A8/"},{"title":"在MIPS CPU中引入微代码","text":"本项目所设计的CPU，与李亚民书中所描述的CPU主要区别是在MIPS CPU中引入了微代码，方便指令集的扩展、编译器的移植和系统编程。 带有微指令的IF级 由原理图可知，主要的修改是加入了： 微指令存储器 uprog 微指令寄存器 uPC 下一个微指令选择器 IF控制器 IF Control 状态寄存器 State 微指令存储器的结构本平台使用的微指令有如下结构： classDiagram class MicroCode{ head 1 head 2 ...... head n code1 () code2 () coden () } class Head{ a JUMP instruction first op in the code section-delay slot } 微指令存储器包括两个部分,头部按照指令号寻址，按顺序存放在存储器中。每个头部域有两行，第一行是一个跳转指令，跳转到对应的代码域，第二行是为了满足延时槽的特点，将代码域的第一行代码放置在延时槽中。 由于微指令的头部有两行，所以在使用指令中的指令号作为地址对微代码存储器寻址时，要先向左位移一位。 如此设计微指令存储器，可以方便寻址，并且最大限度地利用微指令存储器的空间，头部和代码之间没有分割。 微指令存储器的代码域由一个终结指令分割。这个终结指令供IF控制器判断一个微指令是否结束。代码域中可以使用任何一条用硬线逻辑实现的指令，一旦进入微指令状态，微指令中的跳转指令将只修改微指令计数器uPC。每个微指令代码域的最后一条指令（终止符前）不能是跳转指令。 微指令工作过程在不考虑中断或异常的情况下，整个IF阶段实际上分为两个个状态： 正常状态 该状态是指现在没有微指令在执行 伪指令状态 该状态指现在正在执行微指令 与此同时，还要判断两个动作： 正常状态–&gt;微指令状态（enter） 微指令状态–&gt;正常状态（exit） IF的状态由State Register记录，动作由state和正在输入的指令（INS_in, uINS_in）判断。 状态转换 State nState INS_in uINS_in wpc wupc enter out_sl n-&gt;n 0 0 0 x 1 0 0 0 n-&gt;u 0 1 1 x 1 1 1 1 u-&gt;u 1 1 x 0 0 1 0 1 u-&gt;n 1 0 0 1 1 1 0 0 u-&gt;u 1 1 1 1 1 1 1 1 上表中，表头对应原理图中的信号。state=0表示正常状态，state=1表示进入微指令状态。nState表示下一个状态。INS_in=0 表示当前指令以硬件方法实现，INS_in=1 表示当前指令是以微指令方式实现。uINS_in=0表示当前微指令不是结束符，uINS_in=1表示当前微指令是结束符。wpc和wupc分别控制pc和upc的写入使能。 enter为进入微指令状态时的指示信号，为了提高ipc，一旦发现一条指令时使用微指令实现的，就应当直接输出对应的微指令，所以进入微指令状态需要额外处理：微指令存储器的地址直接由enter信号选择为指令中的地址位，微指令存储器输出head中对应的跳转指令，下个时钟周期到来时，该地址+4放入upc，微指令状态延时槽仍然保留。之后的upc由ID阶段的upcSource更新。 out_sl标志输出是微指令存储器内的内容还是指令cache中的内容。当状态发生转换时，n-&gt;u时，若将指令cache中的指令直接传入ID，ID是无法识别的，这个时候因为enter信号的作用，使得该指令对应的微指令已准备好，所以out_sl为1，即当微指令实现的指令出现时，直接传入其对应的微指令。同理，当u-&gt;n时，若将微指令结束符传入ID，ID也是无法识别的，这个时候急需要使out_sl=0选择输出下一条指令。这里还有一个特殊情况，就是当下一个输入的指令也是由微指令实现的，那么实际上IF阶段再一次进入了微指令状态（reenter），这时enter信号选择了新的微指令地址，向ID级输入正确的指令。 关于pc和upc的控制有些复杂，简单来说，就是消耗一条指令，就要写一次pc，消耗一条微指令就要写一次upc。所以可以看到当enter和reenter的情况发生时，等于说既消耗了一条指令，又消耗了一条微指令，所以在这种情况下pc和upc均被写入。 由于延时槽和流水线的存在，使得下一个pc和upc的值的判断变得复杂。而我的设计又希望尽可能地将修改停留在IF级，所以才把pc和upc地写入限制加在了IF级。由于ID级要计算跳转指令的目标地址，所以当前PC或者uPC要传入ID级。改进的IF级中，nPC和nuPC的两个多路复用器，除了+4这一项输入分别对应pc和upc自身外，其余三个输入是相同的，均由ID级直接给出，选择信号也是相同的，也由ID级直接给出。这样做的好处是对于ID级不需要做任何修改，就能实现在微指令状态中的跳转修改upc，而在正常状态下的跳转指令修改pc。考虑下面几种情况中pc和upc的变化。 存储器位置 情况一 情况二 情况三 01 硬件实现指令（非跳转） 跳转指令 硬件实现指令（非跳转） 02 微指令实现指令 微指令实现指令 微指令实现指令 03 硬件实现治指令 跳转指令的目标 微指令实现指令 情况一是比较正常的情况，pc=01，该指令是一个硬件实现指令，所以直接输出，这时候向下一级输出PC。下一个时钟到来，若当前指令是延时槽中的指令，pc等于其目标；若当前指令不处于延时槽中，则pc=pc+4。若跳转的目标是一条普通指令，这种情况比较正常，不涉及到状态切换，在这里不考虑。 当pc=pc+4 或者当前指令是延时槽中的指令，并且跳转的目标刚好是02，这时候要发生状态切换。此时01指令已在ID级，它不是跳转指令，所以pc_source被ID级设置成pc+4。而此时是enter的情况，所以upc被设置成指令中所知的位置，而upc_source和pc_source公用一个信号，所以upc_source也被设置成upc+4.当 下一个时钟周期到来时，由微指令存储器指出的微指令被送入ID，而pc和upc分别被写入为03和微指令头部延时槽的地址。并且IF级向ID级送入upc，供微指令中的跳转指令参考。 当微指令执行结束，也就是upc指向了一个终止符。这时上一条指令（ID级中的指令），由于一定不是跳转指令，所以会把pc_source设置成pc+4，准备向ID级输出pc所指的指令03。下一个时钟到来，upc+4，pc+4=04，而upc指向结束符的下一个指令，脱离结束符的状态。 情况二比较特殊，02位置的指令是一个延时槽。我们考虑这种情况下upc和pc如何变化。pc=01，该指令是一个硬件指令，所以直接输出，这时候向下一级输出pc，此时时钟到来，pc+4=02指向微指令实现的指令。由于ID级是跳转指令，所以ID级将pc_source设置成响应的跳转目标，这里是03。所以时钟到来后，pc=03，upc为指令中所指示的地址+4。可以看出，这种情况实际上和上一种情况是等价的，只是看起来比较特殊。在微指令执行结束后，exit情况也同上一种情况等价。 情况三考虑重进入这种情况。pc=01该指令是一个正常的指令，与情况一相同，在这就不详细分析了。考虑重进入的时刻，将02所代表的微指令送入ID时，pc已被设置为03，此时02中的微指令执行完毕，upc指向终止符，由于此时ID级一定不是跳转指令，因此pc_source被设置成pc+4，由于是重进入，当时钟到来时，IF会选出03所指的微指令中的跳转指令，并将upc设置为03中指令所指的位置+4（微指令头部的延时槽），同时会将pc设置为04. 由对上述三种情况的分析可以知道，我所设计的微指令系统时可以正常工作的。对这些情况的分析是多余的，是因为npc和nupc的值是由ID级的指令决定的。ID级是硬件指令，IF正在处理微指令，这种情况（enter）upc是特殊处理的，并不会影响pc的更新。同理ID级时微指令，IF级正在处理硬件指令，这种情况下upc和pc都直接加4，upc+4后跨越了终结符，其值在下一次进入时在被更新。其余的情况ID级时硬件指令，IF级也是硬件指令或者ID级是微指令，IF级也是微指令，这两种情况与未加入微指令系统的情况相同，只是选择性的对pc和upc的写入进行使能控制。 微指令系统对精确中断的影响在流水线中实现精确中断并不是一件容易的事，尤其是在有分支、有延时槽的情况下。所以笔者不打算采用列举各种情况来分析引入微指令对精确中断的影响，而是试图从更高的层面去思考这个问题，以对终端系统进行针对性的修改。 中断和异常会在ID级或者EXE级到来。ID级的异常，可以是硬件中断，未实现的指令或者是系统调用systemcall。EXE级的异常可以是算术指令的溢出。 该微指令系统对原有精确中断的兼容性分析分析引入微指令对精确中断的影响，要分析IF阶段的取指结果，和那些因素有关。如果中断到来，所保存的现场能够在恢复时确保取出的指令是正确的，那么引入微指令对中断就是兼容的。 时钟到来时，存入IR的指令与下面因素有关： pc的值 upc的值 IF的状态state 初看上一张的真值表，可以发现存入IR的指令还和当前指令是否是硬件实现的，和当前微指令是否是结束符有关。但如果考虑存储器中的内容在是固定的（对于指令和微指令而言，存储器的内容都是固定的），那么当前指令和当前微指令均是由pc和upc决定的。 也就是说，只要保存pc、upc和state，在恢复时就可以恢复这一个时钟到来时所取的指令。 而精确中断的机制，就是根据发生异常的位置，保存中断返回地址取指时的IF状态，因此，对原有终端系统的修改，只需要将过去只保存PC，改为同时保存PC、uPC和state即可。 中断到来对取指的影响如果没有中断机制，IF级的状态转移如上一节的表格所示。但是中断机制的引入使IF级变得更加复杂。 中断相当于强制跳转并保存现场，也就是说中断到来时应当强制更新pc的值。但是IF级所取的下一条指令是由上文所说的pc,upc以及state决定的，而ID给出的中断信号，只能改变PC_INT多路器的选择信号。由此可知，上文中的状态转移过程，若wpc是1，则中断来临时，pc会在时钟到来时更新到中断向量所指的位置。 但有一种特殊情况，若此时IF级处于微指令状态，state=1，并且下一条微指令不是终结符，那么wpc=0，则pc在时钟到来时不会更新为中断向量所指的地址。这会造成错误。 再考虑IF级的输出。IF级输出也由pc,upc以及state决定，如果只对pc进行修改，使得pc在中断来临时强制修改，仍然不能使得中断向量正确向ID级传送。 所以作者在状态寄存器向IF控制器的输入处增加了一个多路器，在中断到来时强制设置state=0，但又不修改state寄存器的内容，让原有的state寄存器内容可以向下传递，使中断控制电路可以正确的保存中断前的状态，与此同时制造enter或者normal的假象，正确处理中断时的跳转问题。 小结这篇文章描述了该自制计算机系统中处理器使如何处理微指令的。本文所描述的方法，可以充分利用硬线逻辑控制器的速度优势，又可以提供一个完整的微指令环境对指令集进行扩充。该方法区别于在很多书中记载的微指令实现方法，不使用微指令解码来控制各个功能部件实现不同的功能，而是直接以硬件实现的指令作为微指令。一条由微指令实现的指令，像一个汇编语言中的宏，如果没有这个微指令模块，这些功能自然可以通过汇编语言的宏来实现。但是，微指令模块的意义仍然存在，经过精心调试的微指令，可以充分利用延时槽、流水线冲突避免等特性来获得比用户自定义的宏更好的性能，也可以减少系统编程时出错的概率，还可以减少可执行代码的体积。在某些必须使用多周期才能实现的硬件指令，把他转换成这样的微指令，只需要付出极少的硬件代价，并且其执行可以高效的利用流水线。 该系统实现的难点有二： IF级的指令选择设计 IF级的指令选择与中断系统的结合和兼容设计 关于该系统的进一步验证，等到具体实现，会用EDA工具仿真实验，届时笔者会把硬件逻辑描述和测试用例同步更新在文章中。","link":"/%E5%9C%A8MIPS-CPU%E4%B8%AD%E5%BC%95%E5%85%A5%E5%BE%AE%E4%BB%A3%E7%A0%81/"},{"title":"总线控制器设计","text":"总线设计总线形式CPU内cache通过总线与其他设备包括RAM、ROM及IO设备（Memory Mapped IO）相连。总线可以有多个主设备，能发起读写请求，这使得该系统拥有DMA能力，其他设备能在总线空闲时抢占直接访问存储设备。这一点是受到ZipCPU启发的，ZipCPU使用一种成熟的总线，在本项目中我计划设计一种简单的总线。总线设计如下： ========address ========data --------request --------ready --------r_w --------clk总线控制器总线控制器设计如下： ------------ request&lt;--| state |--&lt;DMA 0 | |--&gt;Grant 0 | BUS |--&lt;DMA 1 | Controller|--&gt;Grant 1 | | ... | |--&lt;DMA 7 clk&gt;--| |--&gt;Grant 7 ------------控制器负责对总线上所有的主设备请求进行排队，其内部指定的优先级由DMA 0 -&gt; DMA 7递减。被允许的设备，会通过Grant X信号通知主设备，由该信号控制设备连接在总线上的三态门，允许其与总线通讯。当任意设备发起DMA请求时，request输出高电平（该信号与总线上的request连接），通知从设备进行通讯。控制器内部有一个状态机，当有请求发出时的下一个时钟上跳进入busy状态，收到任何一个从设备发送来的ready后的下一个时钟上跳进入idle状态。在busy状态其输出不变，等待该设备通讯结束，总线才空闲。该控制器由bus_control.v实现。总线上的请求发送和数据接收都在一个时钟内完成。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162module bus_control( dma,grant,req,ready,clk); input [7:0] dma; input ready, clk; output [7:0] grant; output req;//-------------------------- Module implementation ------------------------- //registered grant value. reg [7:0] grant_reg; //internal state machine reg state; always @(posedge clk) begin case (state) //Idle state, in this state, if has a req, jump to state busy //and register the grant value, this device is chosen, and //other devices&apos; request can&apos;t change the output. 0: begin if (req) state &lt;= 1; grant_reg &lt;= grant_inner; end //Busy state, in this state, if has a ready, jump to state idle 1: begin if (ready) state &lt;= 0; end endcase end //dma request queue. reg [7:0] grant_inner; always @(*) begin casez (dma) 8&apos;bzzzzzzz1 : grant_inner = 8&apos;b00000001; 8&apos;bzzzzzz10 : grant_inner = 8&apos;b00000010; 8&apos;bzzzzz100 : grant_inner = 8&apos;b00000100; 8&apos;bzzzz1000 : grant_inner = 8&apos;b00001000; 8&apos;bzzz10000 : grant_inner = 8&apos;b00010000; 8&apos;bzz100000 : grant_inner = 8&apos;b00100000; 8&apos;bz1000000 : grant_inner = 8&apos;b01000000; 8&apos;b10000000 : grant_inner = 8&apos;b10000000; default : grant_inner = 8&apos;b00000000; endcase end //When state == 0 (idle), grant is the instant output of code above //When state == 1 (busy), grant is the registered value. //This lets the grant output stable when one device has already been //chosen. assign grant = state ? grant_reg : grant_inner; //The req signal will remain untill ready signal is received. assign req = (|grant) ? 1 : 0;endmodule //bus_control 总线控制器仿真如图: 总线上的设备总线上的主设备，连接总线上address、data、r_w和ready，连接控制器的DMA X和Grant X。发起请求时，将DMA x置高电平，排队成功后，由Grant X信号控制address、data、r_w和ready上的三态门。只有当设备被控制器选中，address、data、r_w才能在总线上输出或者输入，否则这些信号是高阻状态。主设备对其是否被选中是不可知的，当某个主设备发起了请求，他便将其请求的地址、数据、读写情况放在输出端口，而输出端口的三态门是由总线控制器传回的Grant X信号控制的。若其未被控制器选中，该设备无法收到其他设备发送的ready信号，因此处于等待状态。因此在总线空闲的时候，如CPU连续cache命中，其他设备即可使用总线进行DMA请求。 总线上的从设备，其接口应对地址线进行范围判定，地址线选定该设备内地址即认为对该设备发起请求。在被请求的数据准备好后，应将数据输出到总线，并将ready置高电平。 CPU内部的cache控制器是主设备，只能发起请求而不能被请求。大部分的IO设备都可以作为主/从设备,既可以发起请求，也可以作为请求的对象。这样的设备要设计分开的端口，负责接受请求的从设备使用从设备接口，负责发起请求的主设备使用主设备接口。 测试如何设计并单独测试总线这一部分呢？需要设计主从设备接口，主从设备模拟器，并将模拟设备接入总线和控制器进行方针和测试。 从设备模拟首先设计了一个模块dummy_slave来模拟从设备在总线上的行为。这个从设备可以看作一个内存设备，需要几个周期将数据准备好，可以读可以写。这个从设备也可以用作处理器的测试。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120//module for dummy slave devices.module dummy_slave( clk,address, data, request, ready_out, r_w); input clk, r_w, request; input [31:0] address; inout [31:0] data; output ready_out;//-------------------------- Module implementation ------------------------- //dummy memory reg [31:0] mem [0:32-1]; //internal state machine reg [ 2:0] state; //address range reg [31:0] entry_start, entry_end; //internal registers for bus signal reg [31:0] addr_reg,data_reg; reg r_w_reg,selected_reg; initial begin entry_start=32&apos;b0; entry_end =32&apos;b11111; state = 0; //ready signal is Z when idle, ready line should have a tri0 //pulldown resistance. because there&apos;re other devices on the //bus. ready = 1&apos;b0; addr_reg =0; data_reg=0; r_w_reg=0; selected_reg=0; end //selected if request in address range reg selected; always @(*) begin if ((address &gt;= entry_start) &amp;(address &lt;=entry_end) &amp;request ) selected = 1; else selected =0; end //put the ready_out High Z when device is not selected. reg ready; assign ready_out = (selected | selected_reg) ? ready : 1&apos;bz; //implement inout data port. //if device is selected and the request is a read request, this device //will put data onto the bus. In any other condition, the output will be //high Z. assign data = (selected_reg &amp; ~r_w_reg &amp; ready) ? read :32&apos;bz; //read is the continuous read data out. wire [31:0] read; assign read = mem[addr_reg]; //the state machine implements the dummy wait cycles and ready signal. //one dummy operation needs four cycles. always @(posedge clk) begin //If device is in idle state and selected, register address, r_w //and data. if ((state == 2&apos;b00)&amp; selected) begin state &lt;= 2&apos;b01; //pull the ready line low. ready &lt;= 0; //registered the request addr_reg&lt;= address; r_w_reg &lt;= r_w; selected_reg&lt;=selected; if (r_w) begin data_reg&lt;= data; end end //dummy write and read. else if (state == 2&apos;b01 ) begin state &lt;= 2&apos;b10; if (r_w_reg) mem[addr_reg] &lt;= data_reg; end //dummy wait. else if (state == 2&apos;b10) begin state &lt;= 2&apos;b11; end //operation ready else if (state == 2&apos;b11) begin state &lt;= 3&apos;b100; //one cycle ready signal ready &lt;= 1; end //goto idle next cycle, ready for next request else if (state ==3&apos;b100) begin state &lt;= 00; ready &lt;= 1&apos;b0; selected_reg&lt;= 0; r_w_reg &lt;=0; data_reg &lt;=0; addr_reg&lt;=0; end else begin //if device is idle, and there&apos;s no request on this device //clear all internal registers. state &lt;= 00; ready &lt;= 1&apos;b0; selected_reg&lt;= 0; r_w_reg &lt;=0; data_reg &lt;=0; addr_reg&lt;=0; end endendmodule //dummy_slave 这段代码，在quartus中综合生成了一个隐含的调用FPGA片上存储的模块，模块的类型是dual port/single clock RAM.其实我所设计的模型应当是一个单端口RAM，我查看了Altera的手册，也参考了quartus的代码片段,但是综合后的结果仍然是双端口RAM，这个问题让我想不明白。但是综合的结果满足我的设计需求。细想单端口和双端口RAM的区别，这里综合生成的双端口，应是读写端口共用地址线和数据线，但读端口的数据线用三态门控制，仅在写信号无效时才向总线输出。因为我并不清楚Quartus综合的细节，所以在这里我并不纠结为什么编译器没有按照我的要求综合成单端口RAM了，如果有哪位大佬知道原因，欢迎留言。 1234567891011121314151617181920212223242526272829//Quaratus内置的单端口ram代码片段。module single_port_RAM#(parameter DATA_WIDTH=8, parameter ADDR_WIDTH=6)( input [(DATA_WIDTH-1):0] data, input [(ADDR_WIDTH-1):0] addr, input we, clk, output [(DATA_WIDTH-1):0] q); // Declare the RAM variable reg [DATA_WIDTH-1:0] ram[2**ADDR_WIDTH-1:0]; // Variable to hold the registered read address reg [ADDR_WIDTH-1:0] addr_reg; always @ (posedge clk) begin // Write if (we) ram[addr] &lt;= data; addr_reg &lt;= addr; end // Continuous assignment implies read returns NEW data. // This is the natural behavior of the TriMatrix memory // blocks in Single Port mode. assign q = ram[addr_reg]; dummy_slave的仿真结果： 主设备模拟dummy master最终会被Cache、各种DMA设备替代，但是设计一个dummy master仍然有意义，它可以测试总线的功能，尤其是多个主设备同时请求的情况。 dummy master内部是一个请求队列，其将请求送至总线控制器，在控制器允许后将请求送至总线，等待被请求设备的ready信号，取走数据后重新排队，准备下一个请求。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556module dummy_master( clk, request, ready, grant, address, data, r_w); input clk, ready, grant; output request, r_w; output [31:0] address; inout [31:0] data; //dummy requests. reg [31:0] req_addr [0:32-1]; reg [31:0] req_r_w; reg [31:0] req; wire ready_inner; //dummy memory operation. reg [31:0] mem [0:32-1]; reg [4:0] req_num ; //initial dummy operations. initial begin //simulated operation is here. end //internal state machine. reg state; always @(posedge clk) begin case (state) 1&apos;b0:begin if (request) state &lt;= 1; else req_num&lt;=req_num +1; end 1&apos;b1:begin if (ready_inner) begin if (~req_r_w [req_num]) mem[req_num] &lt;= data; state &lt;=0; req_num&lt;=req_num +1; end end endcase end assign request = req [req_num]; //High z when not granted assign address = grant ? req_addr[req_num] : 32&apos;bz; assign r_w = grant ? req_r_w [req_num] : 1&apos;bz; wire [31:0] data_out = req_r_w [req_num] ? mem [req_num] :32&apos;bz; assign data = grant ? data_out: 32&apos;bz; //mask out ready signal when not granted assign ready_inner = grant? ready :1&apos;b0; endmodule //dummy_master 仿真情况如图: 总线事务测试这里设计的test bench在总线上连接了两个DMA设备，分别接入控制器的DMA[0]和DMA[1]因此，DMA[0]有高优先级。总线上同时接入了两个从设备（内存设备），两个内存设备的地址空间分别是32’b0 -&gt; 32’b11111 和 32’b100000-&gt;32’b111111。test bench很简单，将线网连接至各个模块的实例就可以了： 12345678910111213141516171819202122232425262728293031//This module is the test bench for busmodule bus_t( clk,address_o,data_o,request_o,ready_o,rw_o,DMA_o,grant_o ,ready_inner); output [31:0] address_o, data_o; output request_o, ready_o,rw_o,ready_inner; output [7:0] DMA_o, grant_o; input clk; //These wires are internal bus signal wire [31:0] address,data; wire request, ready, r_w; wire [7:0] DMA, grant; //these ports are used by the simulator output assign address_o =address; assign data_o = data; assign request_o = request; assign ready_o = ready; assign rw_o = r_w; assign DMA_o = DMA; assign grant_o = grant; //instances of bus component bus_control bus_control_0 (DMA,grant, request,ready,clk); dummy_slave dummy_slave_a (clk,address,data,request,ready,r_w); dummy_slave_1 dummy_slave_b (clk,address,data,request,ready,r_w); dummy_master dummy_master_a (clk,DMA[0],ready, grant[0],address,data,r_w); dummy_master_1 dummy_master_b (clk,DMA[1],ready, grant[1],address,data,r_w,ready_inner); endmodule 仿真输出为：从仿真结果可以看到模拟的四个设备8个总线事务功能正常。 小结总线的设计，可以隔离CPU和外围设备，甚至可以隔离IO，因为我所要设计的CPU使用memory mapped IO。有了总线，IO设备只是总线上链接的一个设备罢了, 而且可以轻松实现不同的内存映射，连接ROM和RAM。所以总线设计完成，是本项目的一个里程碑，它标志着对开发设计流程的验证。","link":"/%E6%80%BB%E7%BA%BF%E6%8E%A7%E5%88%B6%E5%99%A8%E8%AE%BE%E8%AE%A1/"},{"title":"读完LCC完整源代码","text":"经过了几个月，终于完整的读完了lcc 3.6 的代码，这一阶段工作基本告一段落。对于我来说，lcc是一个十分复杂的软件，虽然我能感受到作者在解释其实现所用的极大努力，但理解起来仍然十分困难。但是当我完整的看完lcc的代码包括其后端后，得到的收获确实很大。通过阅读，基本可以确定对，于我来说，lcc的移植工作技术上是可行的。 LCC的结构先谈一下lcc的结构。原书中的叙述方法，更多的是从功能模块的角度进行叙述的，详细的描述了每一个模块的实现，但是从纵深的角度来看，缺乏对模块之间相互调用相互配合的描述，因此需要阅读时特别留意整个编译程序运行过程中函数之间的调用，才能把握这个复杂的编译程序的整体脉络。 如果从传统的前端-后端角度去看这个编译器，很难去理解。我认为前端-后端这样的分法，实际上是从功能的逻辑层面去考虑的。虽然整个lcc确实在实现上区分了前端和后端，但是在运行过程中，前端和后端的代码始终在相互配合，难以在运行过程明确分离。在我尚未意识到这一点时，阅读代码基本是在关注某个模块的实现，比如语法分析。而这些模块如语法语法分析，实际上是运行过程中比较深得一层，但从这个角度，很难看到阅读编译器的代码如何带给人与仅上编译原理课程不同的好处，填补理论和实践之间的鸿沟。因为这些一个一个的模块，尤其是语法分析，实际上与编译原理的课程内容很相似，事实上，编译原理课程与语法分析这部分的实现重叠很大。 从语法分析到语义分析，这部分内容就开始显现与计算机课程之间的区别了，编译器实现了ANSI C完整的语义，阅读这部分，更像是阅读C语言标准，这里面可以提炼出一些话题进行深入讨论，如C语言中的空指针定义，强制类型转换，函数调用，可变参数函数，参数的估值顺序，结构体中数据的对齐，C语言中复杂的声明等等。从编译器角度看这些问题，的确给人耳目一新的感觉，但是仅仅读到这里，仍然会有许多疑惑尚未被解释。例如函数调用、参数传递，这些都需要和计算平台配合，这些功能的具体实现，还要继续深入了解。我读到这里的时候，最大的疑惑就是程序中的数据是如何和各个标识符相关联的，内存的地址是如何和变量名相关联的。 继续深入下去，lcc的结构就渐渐浮出水面。按照c语言的文法，一个translation unit是c语言程序的基本处理单元，translation unit 包括声明，可以是变量或者是函数的声明，一个c语言文件可以包括若干translation unit。变量的声明，只是处理了标识符、类型、值在编译器内的内部表示。对于变量的初始化，全局变量不产生代码，而是直接将符号和值相互对应起来。只有进行函数定义的时候，才会生成代码。所以分析lcc的整体脉络，需要知道书中讨论的各种功能模块，实际上是以函数定义为单元，往复进行调用。所以分析应从函数定义开始，逐步深入。便可理解一个简单的c语言程序是如何被lcc处理的。 处理函数定义的函数funcdefn第一部分处理要定义的函数的类型，参数列表，处理局部变量和参数（主要是通过后端指定名字，将名字指定为相对于栈的offset），然后对函数定义的compound statement进行语法语义分析，一边分析，一边生成中间语言（森林形式的中间语言），生成中间语言的过程加以优化（如消除公共子式，常量折叠）。然后调用后端function函数，对中间语言进行处理。 function函数，首先根据funcdefn传递过来的参数和局部变量offset计算栈偏移，如果需要，则为生成复制参数的代码（如原来参数由寄存器传入，需要用另一个寄存器或放入栈中），为生成保存函数调用约定中要保存的寄存器的代码做准备，然后调用gencode，对中间语言进行处理。 gencode首先生成对参数进行复制的中间语言，然后根据中间代码森林中树的类型，调用相应的后端函数。Blockbeg对于compound statement中的局部变量以for循环的形式逐个调用local，分配寄存器变量或在栈中划分空间；Local，address节点专门处理临时生成的local变量。gen节点是重中之重，负责生成主要的代码。 gencode—&gt;gen,处理树状的中间语言。这部分书中表13.1描述的较为清晰，prelabel处理已经确定了寄存器的节点的target，_label在树上用树文法与后端模板进行匹配，reduce选择最好的指令输出。prune删去一些不生成指令的子节点，linerize对指令进行排序，最后ralloc对需要寄存器的节点分配寄存器。 然后function 函数开始真正生成代码，首先先输出函数名作为label，作为函数的入口，然后计算framesize和sp，并将sp移动的指令输出（划分栈空间），然后输出保存寄存器的代码，接下来是移动参数的代码。这些工作做完，function调用emitcode函数，生成函数中的语句生成的代码（从已经被标记修减的中间语言树中，按模板生成汇编指令）。最后生成函数的出口，包括恢复保存的寄存器，栈弹出，跳转返回指令。 变量的标识符和值我在阅读代码的时候，十分关注标识符和值是如何关联的。实际上lcc将于C语言的变量名，处理为在数据段中的一个标签。对于未初始化的全局变量，lcc将变量名标签放入bss段；对于初始化的全局变量，lcc根据情况将【变量名标签：值】对放入LIT（read only）或data段，实际的地址生成是汇编器的工作。对于局部变量，则全部表示为栈偏移的形式。这样做的原因是显而易见的：程序运行时的函数调用是动态的，很难确定局部变量的绝对地址，所以使用相对于栈的偏移来解决问题。不同于全局变量，局部变量的初始化是用生成的代码实现的。 对于一些特殊的变量，如数组、字符串的初始化，首先将常量的内容放入LIT段，然后生成一个临时变量保存这个常量内容的标签。 lcc中mips后端的帧结构高地址 -------------------------------- | 调用者的帧 | -------------------------------- |调用者传递的参数 | -------------------------------- —— |局部变量 | | -------------------------------- |被调用者保存的现场 | Framesize ------------------------------- |被调用者调用其他函数时传递的参数| | ------------------------------- ——-----&gt;sp |另一个帧 | 低地址 这里值得一提的是，参数的偏移是相对于sp+framesize的正偏移，因此是从调用者的帧中获取参数的值（参数传递），而局部变量的偏移是相对于sp+framesize的负偏移。 另一处值得说明的是，被调用者如果调用了其他的函数，那么就需要在当前栈中开辟存放outgoing argument的空间，一个函数可以调用很多个其他的函数，这个空间如何确定？在生成函数的代码的过程中，对所有的子函数调用的参数进行统计，得到最大的outgoing argument数量，这里这个空间是这样确定的。 小结了解了lcc的结构，理解了一个简单的c语言程序是如何由lcc一步一步生成汇编语言的，就可以进行移植工作了。 移植工作主要包括：修改指令模板，修改寄存器分配规则，修改函数调用规则，修改各个基本类型的长度和对齐，修改各种名称约定（如各数据段的名字）等。 对于我即将进行的工作，主要的修改应该包括修改类型的长度和对齐（由于cpu取数操作与mips不同），对于乘法除法指令模板应该为函数调用（cpu无乘法器、除法器，这里还需修改clobber函数），对于浮点操作，需要先用编译器编译生成计算函数，然后将指令模板修改为函数调用。修改个数据段名字与汇编器配合。","link":"/%E8%AF%BB%E5%AE%8CLCC%E5%AE%8C%E6%95%B4%E6%BA%90%E4%BB%A3%E7%A0%81/"},{"title":"这只是一个自制计算机系统的项目","text":"该项目是一个自制计算机系统的项目，其目标包括cpu设计、外围的控制器如内存、cache、IO、中断等设计、外设如打印机、存储器等的设计，编译器和操作系统的移植。项目本身并不追求极限性能，和完美的安全性，只是在力所能及的条件下做一些优化。功能完善、易于测试和扩展是本项目的首要目标。 硬件设计目标本项目所使用的CPU为类MIPS架构，硬线逻辑仅支持整数指令，没有硬件乘法器、除法器。在FPGA中综合，不使用任何基于厂商的IP。采用5级流水线，精确中断，集成SDRAM双端口控制器，包含一级指令和数据cache。 CPU的整体结构与李亚民所著的《计算机原理与设计》中的相似，但调整了一些指令的实现，并期望引入微代码来增加CPU的可扩展性，降低编译器移植和系统编程的难度。 编译器移植目标本项目计划移植lcc编译器到该平台，以实现工具链软件。由于本项目所使用的CPU与MIPS结构相似，所以移植基于lcc编译器的MIPS后端md文件进行。由于本项目所使用的CPU与MIPS结构不完全相同，修改lcc的后端的工作可能包括： 熟悉lcc的整体结构,包括其前端和中间代码生成 基于MIPS后端针对该平台进行移植 进行一些力所能及的优化和测试工作 编译器是一个十分复杂的程序，编译器移植工作可能是整个项目中难度最大的部分。 其他工具链软件的设计 对于连接器，本项目认为其在项目初期是可选的 对于汇编器，本项目拟选择一个维护良好的开源汇编器为基础，进行修改 对于预处理器，本项目拟直接使用gcc工具链的C语言预处理器 操作系统的移植目标鉴于MINIX操作系统有详细的资料，并且代码量很少只有几万行，本项目打算在工具链软件调试适当时，进行操作系统的移植工作，目标是将MINIX操作系统移植到该自制平台。这些工作可能包含： Minix 源码的阅读 X86 保护模式的考察 对于该自制平台的保护模式的设计取舍的思考 在不影响兼容性的情况下加入虚拟内存（段式或页式） Minix 的移植工作 从目前来看，minix中平台相关的代码，大量是处理保护模式、中断向量等内容，在硬件相关的设计上可以进行取舍，以简化移植工作。 对于该自制平台的保护模式的设计目标，主要是为了保护操作系统内核的安全，保障操作系统的稳定运行。并不追求完美的进程间的数据隔离。因为Intel的处理器，花费了巨大的精力，实现了其精妙的保护模式，仍然逃不过幽灵、熔毁等旁路攻击。所以在一个科研性质的自制平台上追求完美的数据隔离是不自量力的行为。 Minix3 并不支持分页，使用的是段式存储，这需要对cpu的内存控制进行一些修改。 希望向CPU中加入微代码，也有方便系统编程的目的。可将常用的系统编程指令序列设计成若干由微指令解释的指令，如内存块的复制，中断、异常发生时现场的保存、从中断、异常返回时的现场恢复等。 因为系统编程这一部分，大部分是用汇编语言直接编写，所以编译器并不能帮上忙，若直接定义新指令，可以简化这部分编程的难度，也利于调试。而且这些系统编程指令，往往不是用户程序所使用的，因此编译器可能永远都不知道这些指令的存在，所以引入这些指令可能不需要对编译器的修改。 参考资料该项目是一个长期项目，笔者已经为实施该项目所需要的知识储备准备了多年，并搜集了一些参考资料。 硬件系统的设计 Verilog HDL 数字设计与综合， Samir Lalnitkar, 夏文宇等译。 See MIPS Run, Dominic Sweetman. 这本书很好看，解释了许多重要的概念，例如MIPS中的指令延时槽，内存的保护等。关于这本书还有一些小故事，之前我一直在读这本书，直到最近我才发现这本书有一本中文译本，而且翻译质量非常好，打开一看竟然是我自己的老师屈建勤翻译的，相见恨晚呀！感谢屈老师能为大家提供这么好的学习资料的翻译。 Computer System Architecture, M.Morris Mano. 这本书是很老的书，但是笔者是从这本书里入门的。这本书是笔者刚开始学习计算机系统设计时，经过了很多次试错，最终选定的一本书，写的非常详细。书中从最基本的数字逻辑开始，一直讲到外围电路的设计，没有使用任何基于商业软件的IP，而且最终书里实现了一个很简单的CPU。 计算机原理与设计,李亚民。本项目设计的CPU基于这本书所记载的类MIPS CPU进行修改。笔者非常感谢这本书的作者，这是一本伟大的著作。这本书是我看到过的对处理器设计描述最详尽的著作。在工程上，理论和实践的差距之巨大，无法用语言描述。阅读这本书，从作者精妙的代码中，就可以填补理论和实践之间巨大的鸿沟，这对一个工程师来说会产生一种巨大的满足感。 Fpga4Fun: https://www.fpga4fun.com/. 笔者通过这个网站入门FPGA。 Computer Organization And Design The Hardware/Software Interface. 这是一本经典教材。 软件系统的设计 Operating System Design and Implementation (the Minix Book), Andrew S. Tanenbaum. 这本书简直是学习操作系统的圣经，就是如果认真读代码的话，需要大量的精力。 Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 3A: System Programming Guide, Part 1. Minix书中的代码是基于X86的，要想读明白其平台相关的部分，X86的系统编程手册是不可缺少的材料。关于X86的保护模式，手册里有介绍，但是他的介绍非常繁琐，笔者也是花了很长时间才弄明白，有机会笔者会写一篇博文，详细介绍minix如何使用X86的保护模式。 MOP: Minix Overdocumentation Project,http://www.os-forum.com/minix/boot/index.php. 这个网站的作者，花费了大量的时间，对minix的启动代码进行了逐行分析。只要完整的阅读完这个网站，你就可以知道一台X86 PC在按下电源按钮后，BIOS运行结束后到操作系统加载之间的所有运行情况。但是要做好心理准备，这个过程很复杂，并且理解他需要了解X86的保护模式。Minix书中并没有对其boot monitor的代码进行注释，只是在书中提了一句，说boot monitor十分复杂，不亚于一个小型的操作系统。事实上也确实如此，笔者不才，完整阅读这个网站花费了两个多月的时间。笔者发现这个网站，是在读minix书的过程中，由于书中对其启动器描述的缺乏，使得我对操作系统初始的运行环境有疑惑，进而找到了这个网站，完美地解答了疑惑。这个网站的作者，对笔者也有很深的影响，有时候做一件事不需要考虑太多结果。学习本身就是一件有意义的事情，对于个人如此，对于社会更是如此。这个网站也是我准备这个博客的原因之一，要把自己的学习过程社会化，只要能有一个人从我的经历中得到帮助，本身就是一件很有意义的事情。 Lcc-A Retargetable C Compiler: Design and Implementation. lcc是一个很有名的编译器，它本身是一个文本程序，这本书详细的介绍了其实现的每一个细节。只是笔者水平有限，读起来十分吃力，花了将近四个月时间才读完，掌握了对其修改的能力，也深入了解了ANSI C标准的实现。值得一提的是，这本书有一中文译本，但是很遗憾，不同章节翻译的质量有很大的波动，而且也有一些翻译上的错误，在这里笔者就不推荐了。","link":"/%E8%BF%99%E5%8F%AA%E6%98%AF%E4%B8%80%E4%B8%AA%E8%87%AA%E5%88%B6%E7%B3%BB%E7%AE%97%E8%AE%A1%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%A1%B9%E7%9B%AE/"},{"title":"Cache设计与实现 I","text":"本以为cache的设计并不复杂，但实际设计过程中，我遇到了不少问题，哪怕稍微考虑一下性能问题，cache的设计的复杂度就会提高很多。cache的复杂与cpu指令解码级的复杂不同，CPU指令解码级的信号较多，但这些信号之间并没有太多的关联，更多的是一种纵向的逻辑关系，一条指令对应一种信号组合，因此在设计和分析过程中并没有太多复杂问题。但cache中的信号的逻辑关系更复杂，更难理清，因此我花费了大量的精力去设计。我引入cache的本意并不是单纯为了性能，最主要的原因是cpu内部采用类似哈弗架构，需要分离的指令和数据存储，引入cache可以在CPU视角构成分离的指令和数据存储。但人的野心总是越来越大，当你了解了简单的cache如何设计之后，总是想能不能再一次提高性能，哪怕提高一点点。于是我原本计划的直接映像的cache就变成了现在设计的2路组相联cache。Cache的设计是目前我在整个系统设计过程中遇到的最有挑战性的工作。 Cache 设计时遇到的问题Altera片上存储接口上隐含的寄存器我在cache设计中遇到的问题之一是关于FPGA片上资源的问题，Altera片上存储中几条输入的地址线和数据线都是必须registered的，这就意味着如果使用片上的存储实现cache，IF级PC寄存器实际上应该隐含在I-cache模块内部，EXE和MEM级的流水线寄存器中包含内存地址、数据和控制的部分也应该隐含在D-cache的内部，这实际上给模块化设计带来了困难。（Quartus II Handbook Version 9.1 Volume 1: Design and Synthesis, Chapter 6: Recommended HDL Coding Styles, Inferring Memory Function from HDL Code）如果在cpu设计的过程中忽略这隐含的寄存器，会额外引入至少一个周期的时延。但cache的设计目标是要求在命中的情况下，必须在一个周期给出数据和指令，所以隐含的寄存器必须加入考虑。 Cache的一致性问题另外Altera的片上存储不支持清零（Quartus II Handbook Version 9.1 Volume 1: Design and Synthesis, Chapter 6: Recommended HDL Coding Styles, Inferring Memory Function from HDL Code），这给实现cache清零指令带来了困难：如果使用片上存储来实现cache，不能清零使得实现cache清零指令有困难，而如果没有cache清零指令，缓存同步的问题就需要完全由硬件解决。这又让我思考是否引入总线监听（bus snooping）。有人认为我这样一个单核心单级cache的系统不需要考虑cache一致性的问题，实际上这样的想法不完全正确。考虑这两种情况：一是自修改代码（Self-Modifying code）问题，设想有一操作系统在系统上运行，将某个程序指令从硬盘加载到内存，执行一系列LW操作。操作系统从硬盘读取一串指令，放入内存的某一位置，恰好覆盖了之前内存中的指令。此时若I-cache已经缓存过这一区域，I-cache根本不知道LW指令已经修改了这部分内存数据，这就会引起cache一致性的问题。当CPU跳转到这个区域，I-cache认为命中，实际执行的是旧指令而不是我们通过LW载入的新指令。二是DMA问题，当总线上有DMA请求发生时，DMA设备直接修改某些区域的内存，而Cache并不知道，这同样会引起缓存一致性问题。 对于DMA第二种情况，MIPS处理器采取一种简单粗暴的办法来解决，就是在内存地址空间中单独划出来一部分不被cache索引，每当处理器试图获取这个地址范围内的数据或指令时，都绕过cache，从内存上请求。但是第一种情况要麻烦一些，因为操作系统要频繁的将程序的指令写入内存，我们不能总是把能自我修改的指令部分放入这个绕过cache的地址空间内，这样的话大部分程序都得不到cache的速度优势，还不如不引入cache。 自修改代码 （Self-Modifying Code）如何搞定这第二种情况？是直接在硬件上提供缓存一致性的支持？还是加入一些缓存同步指令在必要的时候由系统软件编写者对cache进行管理，如在操作系统加载新进程的时候手动清空I-cache？我参考了一些资料。 显然不同平台对自修改代码带来的缓存不一致问题的处理方式不同。 X86平台的cache一致性机制比较复杂，从某种意义上讲，只要使用者正常使用处理器，cache对程序员完全透明。 osdev上有人讨论这一点： Everything (including DMA) on 80x86 is cache coherent; except for the following cases &gt;(where you have deliberately “weakened” cache coherency for performance reasons or broken cache coherency): You’re using the “write-combining” caching type in the MTRRs and/or PAT. In this case writes that are sitting in a CPU’s write-combining buffer won’t be seen by other hardware. You’re using non-temporal stores. In this case writes that are sitting in a CPU’s write-combining buffer won’t be seen by other hardware. You’ve seriously messed things up (e.g. used the “INVD” instruction at CPL=0). All hardware trickery (e.g. eDRAM caches, etc) is designed to uphold this. 在看minix代码的时候我就意识到了这一点。我看完整个启动器的代码，里面配合BIOS中断进行了大量DMA操作，从磁盘读取数据到内存，从来没有在代码上下文中出现管理cache的指令。自修改代码在Bootloader中也是随处可见，第一处就出现在master boot record的开头，将BIOS从MBR磁盘第一扇区的主启动记录（Master boot record）512字节的代码通过BIOS中断拷贝到内存后，这段代码将自身复制到其他的区域，并跳转到这里。这事很典型的自修改代码。 123456789! Code from minix 3.1 master boot record: masterboot.s (line 46-53).! Copy this code to safety, then jump to it. mov si, sp ! si = start of this code push si ! Also where we&apos;ll return to eventually mov di, #BUFFER ! Buffer area mov cx, #512/2 ! One sector cld rep movs jmpf BUFFER+migrate, 0 ! To safety 可以清楚的看到，在编写X86汇编程序的时候，程序员不需要考虑这段代码是自修改代码而引起的缓存问题。可是事实上，在rep movs执行时，后续的指令已经加载进入流水线了。显然流水线中的旧指令需要被废弃。 查看X86系统编程手册（Intel software developer’s manual Volume 3A: System Programming Guide 11-18） 11.6 Self-Modifying Code , 可以看到从奔腾四开始，x86处理器已经在流水线和cache硬件上做了一致性机制，只要是使用同一物理地址访问的内存，硬件提供一致性支持。X86不仅在cache中实现了写请求监听，而且还在流水线中废弃已经预取的指令。 11.6 Self-Modifying CodeA write to a memory location in a code segment that is currently cached in the processor causes the associated cache line (or lines) to be invalidated. This check is based on the physical address of the instruction. In addition, the P6 family and Pentium processors check whether a write to a code segment may modify an instruction that has been prefetched for execution. If the write affects a prefetched instruction, the prefetch queue is invalidated. This latter check is based on the linear address of the instruction. For the Pentium 4 and Intel Xeon processors, a write or a snoop of an instruction in a code segment, where the target instruction is already decoded and resident in the trace cache, invalidates the entire trace cache. The latter behavior means that programs that self-modify code can cause severe degradation of performance when run on the Pentium 4 and Intel Xeon processors. In practice, the check on linear addresses should not create compatibility problems among IA-32 processors. Appli- cations that include self-modifying code use the same linear address for modifying and fetching the instruction. Systems software, such as a debugger, that might possibly modify an instruction using a different linear address than that used to fetch the instruction, will execute a serializing operation, such as a CPUID instruction, before the modified instruction is executed, which will automatically resynchronize the instruction cache and prefetch queue. (See Section 8.1.3, “Handling Self- and Cross-Modifying Code,” for more information about the use of self-modifying code.) 各个平台对自修改代码的支持不同。 这篇博客对路由器中广泛使用的MIPS处理器进行了测试，发现其并没有实现自修改代码的缓存同步机制。由Stackoverflow上的讨论,PowerPC也没有实现这个机制，要求系统软件设计者从软件层面维护缓存的一致性。 3.3.1.2.1 Self-Modifying Code When a processor modifies any memory location that can contain an instruction, software must ensure that the instruction cache is made consistent with data memory and that the modifications are made visible to the instruction fetching mechanism. This must be done even if the cache is disabled or if the page is marked caching-inhibited. 我打算如何对自修改代码的缓存进行处理？如果不增加总线监听、流水线回退等机制，只引入清空cache的指令，那就需要系统程序员对自修改代码加以限制，如限定在关中断的情况下修改代码，并在修改代码后引入若干空指令填充流水线，然后使用指令cache清零指令清零cache。这样做降低了硬件设计的难度，但对所有的自修改代码无论其是否在缓存中都需要用控制令填充流水线、清空cache，带来了额外的性能开销。 总线监听实现起来不复杂，但是由于cache变化引起的流水线回退机制和中断的相互作用分析起来有点复杂。是否增加流水线回退机制，取决于能否联合中断，分析清楚处理器的工作状态。当然引入这个cache一致性机制会带来一些额外的好处，比如可以拓展DMA的地址空间，只把单纯的IO放在不经过cache的地址空间即可。 但是，当总线监听、中断、异常和流水线结合起来后，事情变得有点复杂。解决方案是要综合分析自修改代码引起的流水线和cache变化以及其他的一场和中断的组合，设计电路。 精确中断有两种实现方法，一是无论什么时刻中断发生，cpu立刻暂停当前工作，进入终端流程。这个要求较高，需要分析中断发生时ID级的三种情况: ID级是转移指令–中断保存跳转指令的地址，中断返回重新执行 ID级是处在延时槽中的指令–中断保存当前PC，让延时槽中的指令执行完，当前PC就是之前的跳转计算出的地址。 其他情况–中断保存当前PC，与上一种情况类似。 之所以需要考虑这些情况，是因为中断响应过程处理器取指的地址发生变化，而是否是转移指令、是否处在延时槽中对于CPU下一周期取指的地址有影响。 也可以选择等到CPU处在较为稳定的状态时在响应中断。上面可以看到，若发生中断时ID级是转移指令，则流水线中要取消两条指令，待中断返回后再次重新执行。这样做实际上造成了额外的性能开销，我更倾向于等待CPU进入稳定状态时响应中断请求，在ID级是转移指令或延时槽时不响应中断请求。这样做需要在中断控制器中增加一个中断响应机制，在收到响应后应立刻撤销中断请求。 异常的情况要比中断复杂，因为异常往往需要立即处理。一个复杂的CPU应当可以处理多种异常，例如保护模式引入的异常，在取指、读取、存储数据时都可能发生的超过范围，算数运算溢出，指令未实现、syscall指令、还有分页模式引入的异常如tlb缺失，缺页等。目前我尚未打算在我的CPU中实现保护模式和分页，因此目前支持的异常时算数运算溢出、指令未实现和syscall指令。第一个异常是发生在EXE级，后两个是发生在ID级。这三个异常EXE级的异常优先级最高，因为其发生在流水线内较深的位置，而异常处理往往需要重新执行造成异常的指令，所以当多种异常同时出现在流水线的不同阶段中，按照其发生的位置安排优先级处理时得当的方法。异常处理的硬件电路和中断一样，在设计时需要分析不同阶段的异常EPC中要保存的地址，流水线中需要废弃的指令，nPC中的下一个指令地址。具体分析的方法可以看李亚民的书。 其实这里我认为李的书中所描述的CPU工作起来应该有BUG。李并没有设计中断请求和响应的相应机制，默认中断到来就立刻进入中断处理程序。但在其代码中可以清楚看到，中断的优先级是低于异常的，因此当异常和中断同时发生时，由于缺乏中断请求和响应机制，CPU可能在处理异常时错过外部中断，造成IO设备数据丢失。另外，如上文所述，选择立刻处理中断会引入额外的性能开销，使流水线性能下降，因此我倾向于选择等待时机处理中断。 由此可以发现，在流水线系统里实现中断和异常处理并不是一个简单的任务。当你引入额外的硬件提供的cache同步支持时，事情就变得更加复杂。当MEM级发生一次cache写入，若这次写入的地址是已经预取、在流水线中执行着的指令，需要清除Icache中的某行，同时将被影响的指令及其之后的指令从流水线中废弃；若不影响已经预取的指令，则只需要在Icache中清除某行即可。可以将这样的事件当作异常处理，但与异常不同的是，某个引起自修改代码的LW指令，不会产生转移，只是需要在流水线中废弃某些指令。若采用李的书中做法，情况就会变得非常复杂。我若实现硬件cache同步，一定会选择等待时机（处理器无异常、ID无跳转、ID不是延时槽、cache不需要同步）响应中断，这样流水线控制设计只需要分析异常和cache同步即可。 现分析异常和cache同步：如果按照性能考虑，cache同步事件可以在EXE级就计算出来，可以使用旁路将lw指令写入的目标与IF ID 级指令的地址进行比较，判断是否废弃流水线中的指令。由于lw指令不是跳转指令，因此发生cache同步事件时，ID级一定不处在延时槽。如果不考虑性能，可以选择无论IF、ID那个指令需要废弃，只要有一个需要废弃，就直接将IF、ID中的指令作废，重新执行。如果考虑性能，事情就变得复杂了.如果此时ID级是一个跳转指令，那么延时槽中的指令受到cache同步的影响，如果只废弃IF级指令就会引起错误，因为处理器丢弃了ID级的跳转指令计算出的跳转结果，等于ID级跳转指令没有起到作用，所以如果Cache同步影响IF级中的指令，应当判断ID级是否是跳转指令（或syscall），如果是，则不仅要废弃IF还要废弃ID。 这样看来，硬件支持的cache同步机制配合等待时机的中断响应机制，以及立刻处理的异常机制在硬件上可以实现。但是我不打算实现它。为什么呢？ 在之前的设想中，我将原本发生在MEM级的cache事件，前推到EXE级，这很大程度上避免了更多的流水线性能损失。但是这仅仅可以在无分页机制的情况下实现。如果实现分页，则在指令中的地址都是虚地址，EXE级的地址也是虚地址，不能用于前面级流水寄存器中指令地址的比较，而实地址实在查找MEM级中的TLB后才能得到。也就是说上述设计在实现TLB后，就失效了，而设计TLB是本项目的远期计划之一。因此不妨等到完成MMU设计之后，存储控制系统定型，再加入硬件的cache同步支持。 因此得出结论，目前的设计采用软件控制的cache同步，提供硬件Cache清空指令。系统软件编写者在编写自修改代码时，应先关闭中断，在修改完代码后手动清空cache，并使用紧随其后的若干空指令消除cache不同步引起的流水线错误。","link":"/Cache%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"title":"组相联cache设计、实现与测试","text":"我在上一篇文章中，描述了我在设计cache过程中遇到的困难以及产生的想法，提到了fpga片上的RAM隐含的寄存器引起的设计困难，cache同步的问题以及本平台目前对cache同步机制的目标。经过半个月的设计与测试，本系统中将要使用的两路组相联cache（2-way set associative cache）设计完毕并通过测试。这篇文章主要介绍我的设计实现和测试过程，可以作为我所写的代码的文档使用。 cache 设计目标 只需一个时钟周期即可计算出是否cache命中，命中后立刻通知CPU 2路组相联设计 cache写策略为写透（每当有写请求，同时写入cache和内存） cache替换策略为随机替换 允许通过改写参数自定义无需缓存的地址空间供IO和DMA设备使用 允许通过改写参数改变cache的规模 指令和数据cache都是本模块的实例 多个cache实例可以同时访问系统总线 组相联cache的设计与实现组相联cache并不是高大上的技术，在许多参考书中都有介绍，并且在现代处理器中大量使用。但是互联网上有关组相联cache设计与实现的文章并不多，更多的是对其理论概念的描述。看起来组相联cache是很简单的，但这样一个简单的模块，实现起来也需要下点功夫。关于组相联cache的理论，在这篇文章里我就不赘述了，直接切入正题。在本平台中使用的cache原理图如下，图中的信号名称和代码中的信号名称一一对应。 整个cache主要分为四部分，分别是与CPU和总线的接口、内部的存储器组以及控制单元。在这个平台上，指令cache要比数据cache简单，因为指令cache是只读的，因此如果为指令cache和数据cache涉及两个不同的模块会减少对FPGA资源的占用。但是我偷了个小懒，让两个cache共用一个模块，这样大大减少了我的工作量。 与CPU的接口如图中所示，cache与cpu的接口包括： CPU_stall 信号，该信号是由CPU ID级的CU根据两个cache的状态产生的反馈。 CPU_Addr 总线对于指令cache来说，应该直接与NextPC多路器的输出相连，对于数据cache则与EXE级的地址计算结果直接相连。直接与前一级输出而不是与流水线寄存器相连，是为了在使用板载RAM的基础上仍能满足一个周期得到cache命中结果的关键。板载RAM上有内置的寄存器，如果地址输入不与前级输出直接相连，需要至少两个周期才可以知道cache命中的结果，数据要先经过流水线寄存器，再经过板载RAM内部地址寄存器，才能输入结果。 CPU_data 总线对于指令Cache来说应当全部是零，因为指令cache是只读的；对于数据cache应直接连接前一级的输出。 CPU_req 信号，高电平表示需要从cache获取数据或指令。对于I cache而言，该信号应始终是高电平，因为CPU总是需要获取新的地址；而对于D cache，该信号应与前一级输出相连，由CPU的ID级给出的信号控制，因为不是每条指令都需要访问D cache。 CPU_RW 信号标志当前请求是读请求还是写请求。该信号对于I cache而言始终是低电平，因为其是只读的。对于D cache 应直接与前一级输出相连，由ID级控制。 CPU_clr 信号是控制cache内部的valid bits的清零信号，如果该信号为高电平，则下一时钟上跳时清除整个cache的valid bits。我会在ID级实现两个分别清零I cache 和D cache的指令，用这个指令提供软件控制的缓存同步机制。当存在自修改代码时，系统程序员应使用相应的cache清零指令。 data_o 是向CPU输出的准备好的数据。 ready_o 信号表示cache是否准备好。在这里值得说明的是，由于FPGA板载ram上有内置的register，地址输入必须直接与上一级相连，这在某种程度上会引起流水线寄存器设计的割裂。因此，我将所有上一级来的输入信号全部设计成与cache模块直接相连，在模块内部并行的增设寄存器，将与cache相关的流水线寄存器放置在cache内部，并设置了这些寄存器的输出接口如addr_reg_o,供下一级流水线使用。 与总线的接口在总线控制器的设计时我使用了三态门，但是实际综合出的逻辑不能识别高阻态，因此在设计时要考虑对高阻态信号的屏蔽。总线接口包含： BUS_addr 地址总线 BUS_data 数据总线 BUS_RW 读写标志 BUS_req 总线请求标志，该信号应连接总线控制器上的DMA[0:7],根据DMA编号不同实现不同的优先级。 BUS_grant 总线请求许可，该信号从总线控制器发来，当总线请求获得控制器的许可后该信号为高电平，该信号主要参与总线接口三态门的控制。 BUS_ready 信号，高电平标志总线请求结束。 系统总线接口这部分的代码主要是对总线上三态门的控制，逻辑设计要保证总线上其他设备的信号不能干扰本设备。首先要保证当且仅当总线请求被控制器允许后才能向总线发送数据： 123//BUS Interface assign BUS_addr = BUS_grant ? addr_reg : 32&apos;bz; assign BUS_RW = BUS_grant ? CPU_RW_reg : 1&apos;bz; 同时也要确保只收到发送给自己的ready信号,在自己没有总线请求时过滤掉总线的ready信号，保证ready信号线上的高阻态不会影响自己。 1wire ready_in = BUS_grant ? BUS_ready : 1&apos;b0; 总线的数据接口是双向接口，向总线发送写请求时，直接将数据送至数据总线；发起读请求时，应设置为高阻态，直接从数据总线读取数据。 12wire [31:0] data_to_bus = CPU_RW_reg ? data_reg : 32&apos;bz;assign BUS_data = BUS_grant ? data_to_bus : 32&apos;bz; 在读cache不命中、写入或读写不需要cache的地址时，才应像总线发起请求，其他情况让出总线的控制权。 12assign BUS_req = CPU_req_reg &amp;&amp; ((~CPU_RW_reg &amp;&amp; ~CACHE_HIT_R ) ||(CPU_RW_reg &amp;&amp; ~ready_reg)); 值得说明的一点是，由于模块的输入都是上一级的直接输入，寄存器在模块的内部，而且cache的ready信号与cpu的stall信号直接有关，因此控制信号的逻辑需要小心设计，避免出现组合逻辑环。也就是说cache的ready信号应该只和模块内的寄存器的值有关系，不应当与stall信号直接相关。可以看到总线接口的设计遵循了这样的原则。当总线ready时，cache应同时给cpu ready信号，因此cache的ready信号与总线的状态有关，总线的状态也应该仅与寄存器中的值相关。在总线接口部分，所有的控制信号均使用寄存器中的值而不是上一级的直接输入。 cache内部的存储器如图所示，cache内部共有三类存储器，分别存储TAG、有效位(valid bits)和实际请求的数据(RAM)。这个cache模块可以通过改写参数的方式改变cache的规模，cache的规模由索引的宽度INDEX表示,INDEX默认值是7，也就是说内存地址出去字内地址低两位,[INDEX+1:2]这个范围作为七位的索引，因此可索引cache_lines = 2&lt;&lt; (INDEX -1) 行，在TAG中存储的TAG尺寸为tag_size = 32-2- INDEX。 123456//Number of Lines in each group. localparam cache_lines = 2&lt;&lt; (INDEX -1); localparam tag_size = 32-2- INDEX;//default size of the cache is two 512 bytes (128 lines) set, total size is 1KB. parameter INDEX=7; parameter WIDTH=32; 所谓2路组相联，意思是每个index可以索引两组内容，分别存放在A、B两组TAG、valid bits和RAM中，同时对比两组的TAG和Valid bits，得到cache命中结果。对比直接镜像的好处是自由度更大一些，当读取到两个index相同的内存地址时，这两个数据都可以存放在cache中，而在直接镜像的情况下，此时就会发生cache替换。首先定义index和tag： 1234567//vector index is for indexing the whole cache.wire [31:0] addr = CPU_stall ? addr_reg : CPU_addr;wire [INDEX-1:0] index = addr[INDEX+1 :2];wire [INDEX-1:0] index_reg = addr_reg[INDEX+1 :2];//vector tag is used to determined if there&apos;s a cache hit, if not, new tag is written//in the tag storage. wire [tag_size-1:0] tag_reg = addr_reg [31:INDEX+2]; 然后定义TAG、Valid bits和RAM三个存储器。以其中A组为例代码如下： 12345678910111213141516171819202122232425262728293031//Memory for TAG_A.reg [tag_size-1 : 0] TAG_A [0 : cache_lines-1];reg [tag_size-1 : 0] TAG_A_out;always @(posedge clk)begin if (WE_A) TAG_A [index_reg] = tag_reg; TAG_A_out = TAG_A [index];end//valid bits for group A.reg VALID_A_out ;always @(posedge clk)begin if (clr || cache_clr) VALID_A = 32&apos;b0; else if (WE_A) VALID_A[index_reg] =1; VALID_A_out = VALID_A[index];end//memory for group A.reg [31:0] RAM_A [0 : cache_lines-1];reg [31:0] RAM_A_out;always @(posedge clk)begin if (WE_A) RAM_A[index_reg] = RAM_in; RAM_A_out = RAM_A[index];end 这里有一些细节值得说明。index信号是对addr的截取，addr根据当前cpu的状态选择直接从上一级流水输入的地址或是内部寄存的地址。index_reg是从内部的地址寄存器截取的。同时使用两个信号是实现单个周期计算是否cache命中的关键。在上述的三个存储器中，可以看到所有的读接口的索引都使用index，这就保证了在第一个时钟周期上跳，存储器就已经将新读取结果。而对这些存储的修改仅当cache不命中或者写透时才发生，因此写入最早应该发生在下一周期上边沿（因为总线上的设备的响应时间不同，如果总线上的设备能够在当前周期内相应，就是所谓的最早的情况），而这个时候上一级流水输出的信号已经发生改变了，我们并不能阻止这种改变发生，因此所有的写入信号都应该使用内部寄存器的输出（这个内部寄存器就是流水线寄存器）。这里我们使用stall信号来区分cpu的状态，如果此时stall信号为低电平，这说明cpu在处理新指令，index的值是直接从上一级截取的，使用index提供的索引读取cache内部存储判断是否命中，若不命中，则等待总线响应，同时通过ready信号使cpu暂停，此时stall信号为高电平，此后的周期直到缓存准备好，读请求的索引index都被stall信号选择成内部的寄存器输出，可确保在上一请求没完成时继续读取上一索引。这里使用了阻塞赋值语句，对于同时写入和读取同一个地址的情况，读取的结果是写入的新值，综合的结果会加入一些数据旁路来符合我们定义的行为。总是读取新值在我的设计中是必要的，因为当总线准备好时，当前cache也准备好，而此时另一个cache可能尚未准备好正在请求总线，因此cpu要继续等待。此时下一个时钟上边沿，当前cache将新的TAG、Valid bit和RAM写入cache，但是总线上的ready信号随即消失，我们需要在这个时钟上边沿直接取得cache命中信号，让cpu等待时不错过这次请求的cache信号。总是读取新值可以保证在总线准备好后的下一个时钟上边沿就能输出cache命中信号。对于valid bits，由于存在着清零信号，由于altera的板载存储不支持清零信号，所以它会被综合成寄存器。而其他两块存储会被综合成存储器，使用片上的RAM资源。我所设想的设计目标是不用商业IP，与FPGA无关，使用标准的硬件描述语言设计，在没有片上RAM资源的FPGA上这段代码仍然能够综合，不过要消耗大量的寄存器资源。valid bits的读出应该是寄存器而不是assign连续赋值，因为valid bit的输出直接关系到ready信号，而index与stall信号有关，如果使用连续赋值，会产生组合逻辑循环。 针对不需cache的地址范围的特殊设计我所设计的CPU采用memory mapped IO设计，IO使用访存指令来控制。在不提供额外的硬件缓存同步机制的情况下，IO所使用的地址范围会产生cache同步问题。我采取MIPS的做法，在不需要cache的内存空间中绕过cache。具体做法并不复杂。首先使用一个寄存器NO_CACHE判断当前请求的地址是否在不需要cache的范围,该寄存器仅在获取新请求时更新。 12345678reg NO_CACHE;always@(posedge clk)begin if (clr) NO_CACHE &lt;= 0; else if (~ CPU_stall) NO_CACHE &lt;= (addr &lt;= no_cache_end) &amp;&amp; (addr &gt;=no_cache_start);end 然后引入组C作为作为这no cache区域的缓存。设计这组缓存是必要的，原因上面已经介绍了。因为每次对这一地址范围的请求都是cache不命中，都需要从总线上获取数据。总线上的数据仅仅存在一个周期即总线ready的那个周期。如果CPU在这个周期由于其他的原因stall，CPU就会错过总线上的数据，因此我们在这引入额外的一组C，C组没有TAG，valid bits只有一位，RAM也只有一行，在总线ready后，同其他组一样，将总线来的数据写入到RAM同时更新valid bits，为CPU提供cache命中信号。除此之外，绕过cache的机制也很简单，直接在每次新请求发生时，将C组的valid bit清零即可实现每次对no cache区域的访问都不命中。 123456789101112131415reg VALID_C;always @(posedge clk)begin if ((~CPU_stall) || clr ) VALID_C &lt;= 0; else if (WE_C) VALID_C &lt;= 1;endreg [31:0] RAM_C;always @(posedge clk)begin if (WE_C) RAM_C &lt;= RAM_in;end 引入C组的好处是在提供目标功能的同时，简化了控制单元的设计，控制单元只需要根据NO_CACHE寄存器的结果选择相应的信号即可。 各组的更新信号WE_A, WE_B, WE_C为三个组的更新信号。该信号由cache控制器计算，每次更新只有其中一个有效。 cache控制器的设计与实现cache控制器主要解决三方面的问题： 命中信号的逻辑 不命中时的cache替换 写入cache的内容和向CPU输出的内容的选择 cache命中信号ready_o上文中我已提到，BUS上的数据只存在一个周期，但是CPU可能在这个周期因为其他原因暂停。因此我们必须在总线就绪时将数据保存在cache内部。这样的操作自然应该包含BUS的ready信号。引入一个寄存器ready_reg保存从总线上接收的就绪信号, 该寄存器在新请求到来时清零。 12345678reg ready_reg;always @(posedge clk)begin if (~CPU_stall) ready_reg &lt;= 0; else if (CPU_req_reg &amp;&amp; ready_in) ready_reg &lt;= 1;end 针对三个组，每个组都要有单独的命中信号。 123wire HIT_A = tag_reg == TAG_A_out &amp;&amp; VALID_A_out;wire HIT_B = tag_reg == TAG_B_out &amp;&amp; VALID_B_out;wire HIT_C = VALID_C; 可以看到在AB两组的命中信号比对过程中，使用的全部是经过寄存器同步的值，这避免了从cpu_stall到ready_o之间的组合逻辑环。有了各组的命中信号后，就可以为读请求设置命中信号。若请求在no cache范围，则以HIT_C为准，否则与AB的或为准。 1wire CACHE_HIT_R = NO_CACHE ? HIT_C : (HIT_A || HIT_B); 最终的命中信号ready_o还与一些其他的信号相关： 如果此时并没有请求cache（cpu_req_reg == 0)，则应直接给出命中信号，通知CPU继续工作。 如果是写请求，则命中信号应是总线就绪信号与ready_reg的逻辑或。 如果是读请求，则命中信号应是总线就绪信号与读命中信号(CACHE_HIT_R)的逻辑或。123456789always @(*)begin if (~CPU_req_reg) ready_o &lt;= 1; else if (CPU_RW_reg) ready_o &lt;= (CPU_req_reg &amp;&amp; ready_in) || ready_reg; else ready_o &lt;= (CPU_req_reg &amp;&amp; ready_in) || CACHE_HIT_R;end 总线就绪信号使用请求信号进行过滤，防止高阻态的ready干扰。 cache替换控制当读不命中或有写请求时，要进行cache的更新。Cache的更新机制主要解决两个问题： 何时更新？ 更新那一组？ 第一个问题非常简单，当且仅当总线上数据准备好的时候更新cache，因为所有的不命中都需要请求总线。 1wire need_update = BUS_grant &amp;&amp; ready_in; 第二个问题稍微复杂一些。我们使用随机替换策略，首先要设计一个简单的随机数产生器。这里我使用一个计数器作为随机数产生器。 12345reg random;always @(posedge clk)begin random &lt;= random+ 1&apos;b1;end 对于不需要cache的地址范围，处理比较简单。因为组C只有一行，因此直接替换即可。 1assign WE_C = NO_CACHE &amp;&amp; need_update; 对于正常的情况，处理稍微麻烦一些，并不是很多读者想象的直接利用上面描述的随机数产生器。考虑这样的几种情况： 如果一行内的两组槽位，有一个是空的应该，可以直接往空的组填写数据吗？ 很遗憾答案是否定的。 如果两组都是满的，可以直接使用随机替换吗？ 很遗憾答案还是否定的。 为什么不能直接使用随即替换呢？这里有一种特殊情况：如果两组都是满的，A内存放地址0的缓存，B内存放地址8的缓存。此时我需要向地址0再次写入新值。这种情况下能使用随机替换吗？答案当然是否定的。我们应该优先替换TAG相同的组。这种情况下应当将对地址0写入的新值直接放入原来就存放地址0的组A，而不是随机替换组B。这样做除了有性能优势以外，还保证了cache不发生意外地错误。如果直接使用随即替换替换掉了B组，则cache内会有两条关于地址0的记录，在没有额外的措施的情况下，这就直接引起了错误！CPU不知道那个数据是正确的。因此我们确定了使用随机替换时的策略： 若cache内已经存放有该地址的记录，直接替换这一组，不使用随机替换。 否则再看是否有空闲的组，如有则放入空闲组，否则使用随即替换。信号WE_A,WE_B实现了上述逻辑：123456789101112131415161718reg [1:0] group_sel;always @(*)begin //replace group with the same tag first. if (TAG_A_out == tag_reg) group_sel &lt;= 2&apos;b01; else if (TAG_B_out ==tag_reg) group_sel &lt;= 2&apos;b10; //if both group is empty or valid, randomly choose one group. else if (VALID_A_out == VALID_B_out) group_sel &lt;= random ? 2&apos;b01 : 2&apos;b10; //if only one group is empty, choose the empty group. else group_sel &lt;= VALID_A_out ? 2&apos;b10 : 2&apos;b01;end//only when requested address is in cache range, WE_A or WE_B can be active.assign WE_A = (~NO_CACHE) &amp;&amp; need_update &amp;&amp; group_sel[0];assign WE_B = (~NO_CACHE) &amp;&amp; need_update &amp;&amp; group_sel[1]; 需要留意的是，这里使用的控制信号也都是经过寄存器同步过的，这样可以避免组合逻辑环。 向cache内写入的数据和对CPU输出的数据选择我们需要根据读、写请求的不同以及总线的状态，选择向cache内写入的数据RAM_in和向CPU输出的信号data_o.对于RAM_in。读请求时，应选择总线上的数据BUS_data;写请求时应选择CPU从上一级流水线输入的数据data_reg. 1assign RAM_in = CPU_RW_reg ? data_reg : BUS_data; 对于data_o。若是写请求，则不需要向cpu输出数据；若是读请求，当总线准备就绪时，总是选择总线上的数据，若错过了总线的就绪信号，则按照各组的命中情况，选择各组的RAM作为输出。其实现代码如下： 123456789101112131415always@(*)begin if (CPU_RW_reg) data_o &lt;= 32&apos;b0; else if (ready_in) data_o &lt;= BUS_data; else if (NO_CACHE) data_o &lt;= RAM_C_out; else if (HIT_A) data_o &lt;= RAM_A_out; else if (HIT_B) data_o &lt;= RAM_B_out; else data_o &lt;= 32&apos;b0;end 至此这个符合上文所述的设计目标的cache模块设计完毕了。接下来是繁琐的测试工作。 组相联cache的仿真、测试从这篇文章的篇幅来看，这绝不是一个简单的系统，因此其测试也需要仔细考量。本文开头所描述的设计目标应当全部包含在测试中。经过我的思考，确定了测试的流程：对于功能测试，均采用单个cache实例；在通过了功能测试后，使用多个cache实例测试其联合访问总线。这样做是合理的，确保单个cache功能正常，多个cache联合访存正常即可说明达到了设计目标。 针对单个cache的功能测试首先准备测试代码。先定义总线信号： 123wire [31:0] BUS_addr, BUS_data;wire BUS_req, BUS_ready, BUS_RW;wire [7:0] DMA, grant; 然后实例化单个cache和模拟的内存控制器，实例化总线控制器，并将他们连接到总线上。 12345678910//hook up the bus controllerbus_control bus_control_0 (DMA,grant,BUS_req, BUS_ready,clk);//hook up the simulated memorydummy_slave memory(clk,BUS_addr,BUS_data,BUS_req,BUS_ready,BUS_RW);//hook up the simulated instruction cachecache I_cache (CPU_stall,next_pc , data[i] , 1&apos;b1, rw[i], 1&apos;b0, CPU_data, CPU_ready, PC, BUS_addr, BUS_data, DMA[0], BUS_RW, grant[0], BUS_ready, clr, clk, we_a,we_b,we_c,needupdate,tag,hitA,hitB,RAM_A_out); 可以注意到这个cache实例是连接在DMA[0]上的，因此他具有最高的总线优先级。设置模拟的CPU_stall信号: 1wire CPU_stall = clr ? 0: ~(CPU_ready &amp;&amp; 1); 然后就可以使用不同的预设请求序列来测试cache的功能。 初步测试测试了cache的基本功能: 读不命中、读命中 写不命中，连续写不命中，写后读命中 读写no cache范围使用如下的测试序列1234567891011121314151617181920212223242526272829303132333435363738reg [31:0] address [0:7] ;initialbegin address[0]= 0 ; address[1]= 4 ; address[2]= 0 ; //cache hit address[3]= 16 ;//write address[4]= 16 ;//read address[5]= 8 ; //read in no cache zone address[6]= 8 ; //write in no cache zone address[7]= 8 ; //read in no cache zoneendreg [31:0] data [0:7];initialbegin data[0]= 32&apos;hab2112a ; data[1]= 32&apos;hab2112b ; data[2]= 32&apos;hab2112c ; data[3]= 32&apos;hab21123 ; data[4]= 32&apos;hab21124 ; data[5]= 32&apos;hab21128 ; data[6]= 32&apos;hab21129 ; data[7]= 32&apos;hab21121 ;endreg [7:0] rw;initialbegin rw[0]= 0 ; rw[1]= 0 ; rw[2]= 0 ; rw[3]= 1 ; rw[4]= 0 ; rw[5]= 0 ; rw[6]= 1 ; rw[7]= 0 ;end 测试结果如图：从图中可见：读取0，110ns数据准备完毕，130ns进入下个请求，此次请求未命中。读取4，210ns数据准备完毕，230ns进入下个请求，此次请求未命中。读取0，此次请求命中，数据在一周期内准备完毕，总线空闲，250ns进入下个请求。写入16，写请求总是未命中，330ns写入完毕，350ns进入下一个请求。读取16，写后读命中，数据在一周期内准备完毕，总线空闲，370ns进入下个请求。 从图中可见：读取8，请求no cache范围，总是不命中，450ns数据准备好，470ns进入下个请求。写入8，请求no cache范围，总是不命中，550ns数据准备好，570ns进入下个请求。读取8，请求no cache范围，总是不命中，650ns数据准备好，670ns进入下个请求。这一段测试了有限的缓存同步机制。后续三个请求读取0，4，0均命中。基础功能测试完毕。 替换策略测试替换策略的测试稍微繁琐一些。经过思考我决定先将cache的规模缩小，缩小到最小。当cache规模缩到很小的时候，就很容易制造cache替换的模拟，然后进行测试。将参数INDEX改为1，此时cache的规模为两组，每组两行。然后执行序列： 123456789101112131415161718192021222324reg [31:0] address [0:7] ;initialbegin address[0]= 0 ; address[1]= 4 ; address[2]= 28 ; address[3]= 8 ; address[4]= 16 ; address[5]= 4 ; address[6]= 4 ; address[7]= 4 ; endreg [7:0] rw;initialbegin rw[0]= 0 ; rw[1]= 0 ; rw[2]= 0 ; rw[3]= 0 ; rw[4]= 0 ; rw[5]= 1 ; rw[6]= 1 ; rw[7]= 0 ;end 仿真结果如图: 分析上述序列的过程： 时刻 120ns 220ns 320ns 420ns 520ns 620ns 720ns 请求 0 4 28 8 16 4 4 TAG 0 0 11 1 10 0 0 INDEX 0 1 1 0 0 0 0 行/组 0A 1A 1B 0B 0B 1A 1A 策略 TAG相同组 TAG相同组 空的组 空的组 满，随机 满，随机 TAG相同组 由仿真结果可知，替换策略通过测试。 多个cache的联合访存测试单个cache的功能测试完，达到设计目标后，就要测试多个cache联合访问总线的测试。这个测试不单测试cache功能，还要测试总线的功能。首先将两个cache实例分别作为I cache 和D cache连接在总线和总线控制器上： 12345678//hook up the simulated instruction cachecache I_cache (CPU_stall,next_pc , 32&apos;b0 , 1&apos;b1, 1&apos;b0, 1&apos;b0, CPU_data, CPU_ready, PC, BUS_addr, BUS_data, DMA[0], BUS_RW, grant[0], BUS_ready, clr, clk, we_a,we_b,we_c,needupdate,tag,hitA,hitB,RAM_A_out);//hook up the simulated data cachecache D_cache (CPU_stall,exe_address[i],data[i], exe_req[i], rw[i],1&apos;b0,mem_o,mem_ready,, BUS_addr,BUS_data,DMA[1],BUS_RW, grant[1],BUS_ready ,clr,clk); D_cache接在DMA[1]上，表明指令cache的优先级比数据cache的优先级要高。然后设置合适的CPU_stall信号： 1wire CPU_stall = clr ? 0: ~(CPU_ready &amp;&amp; mem_ready); 最后加载模拟的测试请求： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162//for I cachereg [31:0] address [0:7] ;initialbegin address[0]= 0 ; address[1]= 4 ; address[2]= 0 ; address[3]= 4 ; address[4]= 8 ; address[5]= 16 ; address[6]= 0 ; address[7]= 4 ; end//for D cachereg [31:0] exe_address [0:7];initialbegin exe_address[0]= 0 ; exe_address[1]= 20; exe_address[2]= 0 ; exe_address[3]= 20; exe_address[4]= 20; exe_address[5]= 24; exe_address[6]= 24; exe_address[7]= 24;endreg [31:0] exe_req [0:7];initialbegin exe_req[0]=0; exe_req[1]=1; exe_req[2]=0; exe_req[3]=1; exe_req[4]=1; exe_req[5]=1; exe_req[6]=1; exe_req[7]=1;endreg [31:0] data [0:7];initialbegin data[0]= 32&apos;hab2112a ; data[1]= 32&apos;hab2112b ; data[2]= 32&apos;hab2112c ; data[3]= 32&apos;hab21123 ; data[4]= 32&apos;hab21124 ; data[5]= 32&apos;hab21128 ; data[6]= 32&apos;hab21129 ; data[7]= 32&apos;hab21121 ;endreg [7:0] rw;initialbegin rw[0]= 0 ; rw[1]= 1 ; rw[2]= 0 ; rw[3]= 1 ; rw[4]= 0 ; rw[5]= 0 ; rw[6]= 1 ; rw[7]= 0 ;end 仿真结果如图:0-130ns: I cache未命中访问总线，D cache无请求，130nsCPU准备好接收下一个请求。130ns-330ns: I cache未命中优先访问总线，230ns访问完成轮到D cache访问总线，330ns CPU准备好接收下一个请求。330ns-350ns: I cache和Dcache均命中，总线空闲，350ns CPU准备好接收下一个请求。350ns-450ns： I cache未命中 访问总线，D cache命中，450ns CPU准备好接收下一个请求。450ns-650ns: I cache未命中优先访问总线，550ns访问完成轮到D cache访问总线，650ns CPU准备好接收下一个请求。650ns-850ns: I cache未命中优先访问总线，750ns访问完成轮到D cache访问总线，850ns CPU准备好接收下一个请求。850ns-950ns: I cache命中， D cache未命中，访问总线，950ns访问完成，CPU准备好。950ns-970ns: I cache、D cache均命中，CPU在一个周期内准备好。970ns-990ns: I cache、D cache均命中，CPU在一个周期内准备好。 由仿真结果可知，两个cache可以按照优先级共同访问系统总线，而且cache的存在对CPU的性能提升很有帮助。我们总线上挂的模拟内存控制器需要5个周期100ns事件才能将数据准备好。30-990ns的时间内，共48个时钟周期，共完成了9条指令。若cache不存在，每条指令需要10个周期才能完成，最多才能执行4.8条。 已知问题我的总线设备和总线控制器不知道如何解决对设备中没有的地址的访问。如果访问的地址不在设备的范围内，系统就会出现异常。目前的解决方案：无，程序不应访问不在设备范围内的地址。 小结我在实现过程中遇到的最大的问题莫过于组合逻辑环。因为FPGA片上存储的特性（隐含的寄存器），我必须在模块的某些输入上直接绕过流水线寄存器，并依据CPU_stall进行选择，这就非常容易引入从CPU_stall信号到ready_o信号之间的逻辑环。我在这个问题上花了很长时间进行调试，最后总结出，只在片上RAM的地址输入上使用这个由stall信号选择的输入，其余的控制电路均要使用经过寄存器同步的输入。因为片上ram读取结果是由寄存器同步的，因此这样绕过流水线寄存器不会引起组合逻辑环。组相联cache在各种教科书中都有提及，但其篇幅往往不长，往往并不能描述其具体实现。我做这件事，写这些文章，一是满足自己的欲望，二是希望能够通过动手实践来填补理论和实践之间巨大的鸿沟，这篇文章的篇幅就很能说明问题。组相联cache设计完之后，我的欲望再一次膨胀了。是否加入分支预测，撤掉有些过时的延时槽？是否可能实现简单的超标量流水线？这些还是等到整体设计完再说吧。","link":"/%E7%BB%84%E7%9B%B8%E8%81%94cache%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/"}],"tags":[{"name":"development log","slug":"development-log","link":"/tags/development-log/"},{"name":"computer architecture","slug":"computer-architecture","link":"/tags/computer-architecture/"},{"name":"home brew computer system","slug":"home-brew-computer-system","link":"/tags/home-brew-computer-system/"},{"name":"cpu design","slug":"cpu-design","link":"/tags/cpu-design/"},{"name":"micro program","slug":"micro-program","link":"/tags/micro-program/"},{"name":"compiler","slug":"compiler","link":"/tags/compiler/"},{"name":"summarize","slug":"summarize","link":"/tags/summarize/"}],"categories":[{"name":"home brew computer system","slug":"home-brew-computer-system","link":"/categories/home-brew-computer-system/"}]}